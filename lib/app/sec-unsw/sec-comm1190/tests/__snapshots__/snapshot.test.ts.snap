// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`app(unsw::comm1190).snapshot > should match snapshots 1`] = `
"<style id="test-styles"></style><div class="widget-container document" id="test-widget-0"><style id="test-widget-0-styles"></style><div class="main container"><h1>COMM1190 notes</h1><details name="week" class="dashbox"><summary><h2>Week 1</h2></summary><div class="container pre--container"><div class="c2 pre--c2"><div class="container pre--container"><h3>Types of Analytics</h3><dl><dt>Descriptive Analytics</dt><dd>What happened?</dd><dt>Diagnostic Analytics</dt><dd>Why did it happen?</dd><dt>Prescriptive Analytics</dt><dd>Why should be done about it?</dd><dt>Predictive Analytics</dt><dd>What is likely to happen?</dd></dl></div><div class="container pre--container"><h3>Histogram basics</h3><dl><dt>Symmetric</dt><dd>Not skewed</dd><dt>Bimodal</dt><dd>Two peaks</dd><dt>Multimodal</dt><dd>Multiple peaks</dd><dt>Right skewed</dt><dd>Extreme values on the right side, concentrated on the left</dd><dt>Left skewed</dt><dd>Extreme values on the left side, concentrated on the right</dd></dl></div></div></div></details><details name="week" class="dashbox"><summary><h2>Week 2 ‚Äî Descriptive Stats</h2></summary><div class="container pre--container"><h2>Business Analytics Framework</h2><div class="container pre--container"><h3>Types of data</h3><ul class="no-item-padding" style="margin-block: 0px;"><li>Quantitative <ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Discrete</strong>: limits to how small a change can be</li><li><strong>Continious</strong>: no limit to how small a change can be</li></ul></li><li>Qualitative <ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Nominal</strong>: describes charteristics</li><li><strong>Ordinal</strong>: ranked but descriptive data (neutral, happy, very happy)</li></ul></li></ul></div><div class="container pre--container"><h3>Types of Datasets</h3><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Cross-sectional</strong>: for each subject we collect one or more variable</li><li><strong>Time series</strong>: for one subject we collect the same variables over different points in time</li><li><strong>Panel Data</strong>: for each subject we collect the same variables over different points in time</li><li><strong>Textual Data</strong></li><li><strong>Image Data</strong></li><li>etc</li></ul></div><div class="container pre--container"><h3>Dimentions of Data</h3><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Completeness</strong>
            data is comprehensive and meets expectations
          </li><li><strong>Consistency</strong>
            data across all systems/sourced from different
            places reflects the same information
          </li><li><strong>Conformity</strong>
            data is following a set of standard data definitions
            like data type, size and format.
          </li><li><strong>Accuracy</strong>
            data reflects the real world object OR an event being
            described.
          </li><li><strong>Integrity</strong>
            all data in a database can be traced and
            connected to other data.
          </li><li><strong>Timeliness</strong>
            informaiton is available when it is expected or needed.
          </li></ul></div><div class="container pre--container"><h2>Data issues</h2><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Formatting issues</strong></li><li><strong>Data type issues</strong></li><li><strong>Missing values</strong></li><li><strong>Outliers</strong></li></ul></div><div class="container pre--container"><h3>Techinical Correctness</h3><p>data is techinically correct when</p><ol class="no-item-padding" style="margin-block: 0px;"><li>It can be directly recognized as belonging to a certain variable</li><li>It is stored in a data type that represents the value domain of the real world variable</li></ol></div><div class="container pre--container"><h3>Missing Data</h3><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Listwise Deletion</strong><ul class="no-item-padding" style="margin-block: 0px;"><li>Remove records with missing values in any variable</li><li>
              Straightforward but can lead to significant data loss and bias if
              the missing data are not completly random
            </li></ul></li><li><strong>Meand/Median/Mode Imputation</strong><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Continious</strong>: Replace missingg value with the mean</li><li><strong>Skewed Continious</strong>: Replace with median</li><li><strong>Categorical Variables</strong>: Replace with Mode</li><li>Simple but can underestimate variablility and lead to baised estimates</li></ul></li></ul></div><h3>Summary statistics</h3><h4>Mean ‚Äî location measure</h4><math display="block"><mrow><msub><mi>ŒºÃÇ</mi><mi>x</mi></msub><mo>=</mo><msup><mi>T</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>T</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow></mrow><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math><h4>Variance ‚Äî dispersion measure</h4><math display="block"><mrow><msubsup><mi>œÉÃÇ</mi><mi>x</mi><mn>2</mn></msubsup><mo>=</mo><msup><mi>T</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>T</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>ŒºÃÇ</mi><mi>x</mi></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></math><h4>Standard deviation</h4><math display="block"><mrow><msub><mi>œÉÃÇ</mi><mi>x</mi></msub><mo>=</mo><msqrt><mrow><msup><mi>T</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>T</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>ŒºÃÇ</mi><mi>x</mi></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math><p>The standard deviation is the most common measure of risk</p><h4>Skewness</h4><math display="block"><mrow><msub><mi>Œ≥ÃÇ</mi><mi>x</mi></msub><mo>=</mo><msup><mrow><mo>(</mo><mi>T</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>T</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>ŒºÃÇ</mi><mi>x</mi></msub></mrow><msub><mi>œÉÃÇ</mi><mi>x</mi></msub></mfrac><mo>)</mo></mrow><mn>3</mn></msup></mrow></math><ul class="no-item-padding" style="margin-block: 0px;"><li>…£ÃÇ &lt; 0 means left skew</li><li>…£ÃÇ = 0 means no skew</li><li>…£ÃÇ &gt; 0 means right skew</li></ul><h4>NTiles</h4><p>data is sorted and split into equally sized portions</p><ul class="no-item-padding" style="margin-block: 0px;"><li>Quantilies ‚Äî 4 portions</li><li>Percentiles ‚Äî 100 portions</li></ul><p><strong>Interquartile range (IQR)</strong>
        is the difference between the 3rd and 1st quartile
      </p><h3>Dealing with Outliers</h3><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Drop the record</strong>
          Completely remove the record
        </li><li><strong>Winsorization</strong>
          Cap your outliers data, by limiting the range (idk).
        </li><li><strong>Imputation</strong>
          Assign a new value (as arbitrary as it sounds)
        </li></ul><h2>Data Privacy</h2><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Notice</strong>:
          Inform users about privacy policy, privacy protection
          procedures e.g. who will be collecting dat, how data
          will be collected, who owns data.
        </li><li><strong>Choice and consent</strong>:
          Consent from indivisuals about the collection,
          use, disclosure, and retention of their information.
        </li><li><strong>Use and retention</strong>:
          data should be retained and projected according to
          law or business practices required e.g. the length
          or business practices required e.g. length of data
          retention; avoid secondary use of data for other
          purposes.
        </li><li><strong>Access</strong>:
          provide acceess to indivisuals with the access to review,
          update, and modify the data about their personal information.
        </li><li><strong>Protection</strong>:
          data is sued only for the purpose stated; de-identifiable
          of sensitive infomraiotn; users have the right to opt out
          for the user of their data.
        </li><li><strong>Enforcement and Redress</strong>:
          provide channels for indivisuals to report provide
          feedback, or complain.
        </li></ul><h2>Data Security</h2><h3>Austrlian Security principals</h3><p>Protect consumer against</p><ol class="no-item-padding" style="margin-block: 0px;"><li>Misuse</li><li>Interference</li><li>Loss</li><li>Unauthorised Access</li><li>Unauthorised Modification</li><li>Unauthorised disclosure</li></ol></div></details><details name="week" class="dashbox"><summary><h2>Week 3 ‚Äî Data Communication 1</h2></summary><div class="container pre--container"><h3>Pop out effect</h3><p>Reserving constrasting element to the most releveant piece of info</p><h3>Data vis categories</h3><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Declaritive</strong><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Conceptual</strong> Idea Illustration</li><li><strong>Data Driven</strong> Everyday data visualisation</li></ul></li><li><strong>Exploratory</strong><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Conceptual</strong> Idea Generation</li><li><strong>Data Driven</strong> Visual Discovery</li></ul></li></ul><h4>Dimensions</h4><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Conceptual</strong>: "Ideas", with the goal to "simplify" &amp; "teach".</li><li><strong>Data Driven</strong>: "Statistics", with the goal to "inform" &amp; "enlighten".</li><li><strong>Declaritive</strong>: "Reporting", with the goal to "affirm".</li><li><strong>Exploratory</strong>: "Interacting", with the goal to "discover" &amp; "corroborate".</li></ul><h3>Elements of excellent charts</h3><ul class="no-item-padding" style="margin-block: 0px;"><li>clear</li><li>easy to understand</li><li>impactful</li><li>tells a story</li><li>accurate</li><li>visually engaging</li></ul></div></details><details name="week" class="dashbox"><summary><h2>Week 4 ‚Äî Predictive Analytics, Part 1</h2></summary><div class="container pre--container"><h3>An Introduction</h3><ul class="no-item-padding" style="margin-block: 0px;"><li>A vast set of tools for understanding data</li><li>Other names used to refer to similar tools, machine learning, predictive analytics, statistical learning</li><li>Techniques making significant impact on businesses organisations and daily life</li><li>Historically started with classical linear regression techniques</li><li>Contemporary extensions faciliated by powerful computation techniques and data/software availablity</li></ul><p>As a function</p><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Input</strong>: Independent Variable</li><li><strong>Output</strong>: Dependant Variable</li></ul><div class="c2 pre--c2"><div class="container pre--container"><h4>Explanation Models</h4><ul class="no-item-padding" style="margin-block: 0px;"><li>Understand how Y is affected by X</li><li>Models tend to be simpler</li><li>Which predictors do we add? How are they related?</li></ul></div><div class="container pre--container"><h4>Prediction Models</h4><ul class="no-item-padding" style="margin-block: 0px;"><li>Predict outcomes of Y given X</li><li>Models tends to be more complex</li><li>What it means isn't as important, it just needs acurate predictions</li></ul></div></div><h3>Modelling framework</h3><math display="block"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>f</mi><mspace width="2px"></mspace><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>+</mo><msub><mi>Œµ</mi><mi>i</mi></msub></mrow></math><ul class="no-item-padding" style="margin-block: 0px;"><li>Outcome of interest y_i, relates to inputs x_i</li><li>Eplison is the unexplained part of the error term</li></ul><h3>Ordinary Least Squares</h3><math display="block"><mrow><msub><mi>≈∑</mi><mi>i</mi></msub><mo>=</mo><msub><mi>Œ≤ÃÇ</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>2</mn></msub><msub><mi>X</mi><mn>2</mn></msub><mo>+</mo><mo>‚Ä¶</mo><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub></mrow></math><h3>Examples of uses of Predictive models</h3><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Quality Control</strong> - fraud detection of junk email</li><li><strong>Inventoary management</strong> - sales forecasting</li><li><strong>Risk analysis</strong> - churning/staff turnover/readmission</li><li><strong>Market segmentation</strong> - who are my least/most satisfied customers</li></ul><h4>Confidence interval</h4><math display="block"><mrow><msub><mi>Œ≤ÃÇ</mi><mn>1</mn></msub><mo>¬±</mo><mn>1.96</mn><mo>¬∑</mo><mrow><mtext>SE</mtext><mspace width="2px"></mspace><mrow><mo>(</mo><msub><mi>Œ≤ÃÇ</mi><mn>1</mn></msub><mo>)</mo></mrow></mrow></mrow></math><h4>Hypothesis Test</h4><math display="block"><mtable><mtr columnalign=""><mtd columnalign=""><mtd><mover><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mspace width="8px"></mspace><msub><mi>Œ≤</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn><mspace width="16px"></mspace><mtext>There is no relation</mtext></mrow><mtext>Null hypothesis</mtext></mover></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mspace width="8px"></mspace></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mover><mrow><msub><mi>H</mi><mn>1</mn></msub><mo>:</mo><mspace width="8px"></mspace><msub><mi>Œ≤</mi><mn>1</mn></msub><mo>‚â†</mo><mn>0</mn><mspace width="16px"></mspace><mtext>There is some relation</mtext></mrow><mtext>Alternative Hypothesis</mtext></mover></mtd></mtd></mtr></mtable></math><h4>P Values</h4><ul class="no-item-padding" style="margin-block: 0px;"><li>
          The P value is the probability of finding the observed,
          or more extereme results when the null hypothesis (H0)
        </li><li>
          The lower the value the greater the significance of the observed difference
        </li><li>
          P values of 0.05 of lower are generally considered statistically significant.
        </li></ul><h4>Model Fit</h4><math display="block"><mrow><mover><mrow><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>n</mi><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="4px"></mspace><mo>-</mo><mspace width="4px"></mspace><mi>»≥</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow><mtable><mtr columnalign=""><mtd columnalign=""><mtd><mtext>Total Sum of squares (TSS)</mtext></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mtext>Total Variance of Response</mtext></mtd></mtd></mtr></mtable></mover><mo>=</mo><mover><mrow><mspace width="8px"></mspace><mrow><mo>(</mo><mrow><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>n</mi><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="4px"></mspace><mo>-</mo><mspace width="4px"></mspace><mi>»≥</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>-</mo><mrow><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>n</mi><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="4px"></mspace><mo>-</mo><mspace width="4px"></mspace><mi>≈∑</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>)</mo></mrow><mspace width="8px"></mspace></mrow><mtable><mtr columnalign=""><mtd columnalign=""><mtd><mtext>Variablity explained by</mtext></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mtext>the regression (ESS)</mtext></mtd></mtd></mtr></mtable></mover><mo>+</mo><mover><mrow><mspace width="8px"></mspace><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>n</mi><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="4px"></mspace><mo>-</mo><mspace width="4px"></mspace><mi>≈∑</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow><mtable><mtr columnalign=""><mtd columnalign=""><mtd><mtext>Residual sum of</mtext></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mtext>Squares (RSS)</mtext></mtd></mtd></mtr></mtable></mover></mrow></math><math display="block"><mrow><mover><msup><mi>R</mi><mn>2</mn></msup><mtext>R Squared</mtext></mover><mo>=</mo><mfrac><mrow><mi>TSS</mi><mo>-</mo><mo>RSS</mo></mrow><mi>TSS</mi></mfrac><mo>=</mo><mfrac><mo>RSS</mo><mi>TSS</mi></mfrac></mrow></math><h3>Professional reporting of results</h3><ul class="no-item-padding" style="margin-block: 0px;"><li>Use descriptive names</li><li>Avoid scientific notation</li><li>Avoid numerical clutter</li><li>Report P-Values</li><li>Report sample size &amp; measure of fit</li><li>Helpful to include mean of dependent variable</li></ul><h3>Model Building</h3><ul class="no-item-padding" style="margin-block: 0px;"><li>Simplified and "build &amp; improve" step for regression<ul class="no-item-padding" style="margin-block: 0px;"><li>Specify Model</li><li>Estimate Parameters</li><li>Interpret results &amp; draw conclusions</li></ul></li><li>First step<ul class="no-item-padding" style="margin-block: 0px;"><li>Use subject matter expert to select variables</li></ul></li><li>Ensure residuals are evenly dispersed and clustered around zero</li><li>Consider using log transformations</li></ul></div></details><details name="week" class="dashbox"><summary><h2>Week 5 ‚Äî Predictive Analytics, Part 2</h2></summary><div class="container pre--container"><div class="container pre--container"><h3>Prediction Certainty</h3><math display="block"><mrow><msub><mi>≈∑</mi><mi>i</mi></msub><mo>=</mo><msub><mi>Œ≤ÃÇ</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>2</mn></msub><msub><mi>X</mi><mn>2</mn></msub><mo>+</mo><mo>‚Ä¶</mo><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub></mrow></math><div class="c2 pre--c2"><div class="container pre--container"><h4>Confidence interval</h4><math display="block"><mtable><mtr columnalign=""><mtd columnalign=""><mtd><mrow><msub><mi>≈∑</mi><mi>new</mi></msub><mo>¬±</mo><msub><mi>t</mi><mrow><mi>Œ±</mi><mo>/</mo><mn>2</mn></mrow></msub><mo>¬∑</mo><msub><mi>SE</mi><mi>mean</mi></msub></mrow></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mspace width="0px"></mspace></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mrow><msub><mi>SE</mi><mi>mean</mi></msub><mo>=</mo><msqrt><mrow><msup><mi>SE</mi><mn>2</mn></msup><mo>¬∑</mo><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>+</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>X</mi><mi>new</mi></msub><mo>-</mo><mi>XÃÑ</mi><mo>)</mo></mrow><mn>2</mn></msup><mrow><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>n</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>-</mo><mi>XÃÑ</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow></mfrac><mo>)</mo></mrow></mrow></msqrt></mrow></mtd></mtd></mtr></mtable></math><p>Where:</p><ul class="no-item-padding" style="margin-block: 0px;"><li>SE^2 is the variance of the resideuals from the regression model.</li><li>SE_mean is the standard error of the mean prediction.</li></ul></div><div class="container pre--container"><h4>Prediction Intervals</h4><math display="block"><mtable><mtr columnalign=""><mtd columnalign=""><mtd><mrow><msub><mi>≈∑</mi><mi>new</mi></msub><mo>¬±</mo><msub><mi>t</mi><mrow><mi>Œ±</mi><mo>/</mo><mn>2</mn></mrow></msub><mo>¬∑</mo><msub><mi>SE</mi><mi>pred</mi></msub></mrow></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mspace width="0px"></mspace></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mrow><msub><mi>SE</mi><mi>pred</mi></msub><mo>=</mo><msqrt><mrow><msup><mi>SE</mi><mn>2</mn></msup><mo>¬∑</mo><mrow><mtext>Var</mtext><mspace width="2px"></mspace><mrow><mo>(</mo><msub><mi>≈∑</mi><mi>new</mi></msub><mo>)</mo></mrow></mrow></mrow></msqrt></mrow></mtd></mtd></mtr></mtable></math><p>Where:</p><ul class="no-item-padding" style="margin-block: 0px;"><li>y_new is the predicted value for the new observation.</li><li><strong>t_alpha/2</strong> is the critical value from the t
                distribution with n - p - 1 degrees of freedom (where n is the
                number of data pointss, and p is the number of predictors),
                corresponding to the confidence level (e.g. 95% confidence level).
              </li><li><strong>SE_pred</strong> is the standard error of the prediction</li></ul></div></div><h4>Prediction Accuracy</h4><p>Quality of Fit: Mean Squared Error</p><math display="block"><mrow><mi>MSE</mi><mo>=</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>n</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><mrow><mi>ùëìÃÇ</mi><mspace width="2px"></mspace><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>=</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mi>n</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><msub><mi>≈∑</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></math><p>
          This should be small if predicted response are close
          to the true responses.
        </p><p>
          Note if the MSW is computed using the data used to fit the model
          (which is called the training data) then this is more accuratedly
          referred to as the training MSE or within sample MSE. Some
          potential problems with that:
        </p><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Bias in model selection</strong>:
            Relying solely on training MSW can lead to biased model selection,
            favoruing more complex models that fit the trainging data well but
            fail to preform adequately on validation or test datasets.
          </li><li><strong>NOt reflective of real world perfmance or lack of generalizablity</strong>:
            The training MSW reflects perforance only on the training set and
            does not provide insight into how the model behaves in real-world
            applications where the data may vary. Training MSW does not account
            for how well the model will perform on a differnt dataset. It may
            provide a flase sense of security regarding the models predictive power.
          </li></ul></div><hr><div class="container pre--container"><h3>Countering Overfitting</h3><p>
          One approach to couter overfitting is to add a penalty
          for complexity when evaluating fit in original data.
          This is more or less how the Adjusted R Squared works.
        </p><math display="block"><mrow><msub><mi>MSE</mi><mtext>test</mtext></msub><mo>=</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><munderover><mi>‚àë</mi></munderover><mspace width="4px"></mspace><mrow><mspace width="0px"></mspace><mrow><mi>i</mi><mo>‚àà</mo><mtext>Test Data</mtext></mrow></mrow></mrow><msup><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><msub><mi>≈∑</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></math><p>
          Another approach is to split data into a training dataset
          and a testing dataset. This works by evaluating the prediction
          performance on unseen data.
        </p></div><hr><div class="container pre--container"><h3>Machine Learning</h3><p>
          Machine learning is the study of algorithms applied to
          data focussing on prediction &amp; classification. How they
          stand out as forms of predictions is they: <br><br><ul class="no-item-padding" style="margin-block: 0px;"><li>
              Aren't just defined as formulars but also as algorithms.
            </li><li>
              For that reason they can be automated and produce results
              without intervention.
            </li><li>
              Due to the emphasis being placed on prediction much less
              is placed on explaination, which can make them difficult
              to interpret.
            </li></ul>
        </p></div><hr><div class="container pre--container"><h3>Regression trees</h3><p>
          Tree-based methods involve segmenting inputs
          into mutually exlusive &amp; exhaustive regions.
          <br><br><ul class="no-item-padding" style="margin-block: 0px;"><li>Branches connect internal nodes to a terminal node (leaf)</li><li>Branches formed by binary splits at each node.</li><li>Each leaf assigned a prediction (e.g. mean outcome within branch)</li></ul>
        </p></div><hr><div class="container pre--container"><h3>Regression vs Classification</h3><div class="c2 pre--c2"><div class="container pre--container"><h4>Regression</h4><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Y</strong> is quatitative, continious</li><li>
                Exmaples: sales predictions, house prices predictions,
                stock price modelling, model employment satisification.
              </li></ul></div><div class="container pre--container"><h4>Classification</h4><ul class="no-item-padding" style="margin-block: 0px;"><li><strong>Y</strong> is qualitative, nominal</li><li>
                Exmaples: Fraud detection, face recognition,
                whether someon will default of not on debt,
                employment attrition.
              </li></ul></div></div></div></div></details><details name="week" class="dashbox"><summary><h2>Week 7 ‚Äî Predictive Analytics, Part 3</h2></summary><div class="container pre--container"><div class="container pre--container"><h3>Classification</h3><p>
          Models like Linear regression don't really make as
          much sense for classifications problems. For binary
          cases we can use <strong>Logisitic Regression</strong>.
        </p><p>
          The logisitic regression models the probability that Y
          belongs to a paritcular cateogry p(X).
        </p><math display="block"><mrow><mrow><mi>p</mi><mspace width="2px"></mspace><mrow><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mspace width="2px"></mspace><mrow><mo>(</mo><mi>Y</mi><mo>,</mo><mspace width="4px"></mspace><mo>=</mo><mo>,</mo><mspace width="4px"></mspace><mn>1</mn><mo>,</mo><mspace width="4px"></mspace><mo>|</mo><mo>,</mo><mspace width="4px"></mspace><mi>X</mi><mo>)</mo></mrow></mrow></mrow></math><p>
          Since p(X) is a probabily, it takes values between zero
          and one. In logistic regression, we use the
          logistic function:
        </p><math display="block"><mrow><mrow><mi>p</mi><mspace width="2px"></mspace><mrow><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mtext>exp</mtext><mspace width="2px"></mspace><mrow><mo>(</mo><mrow><msub><mi>Œ≤ÃÇ</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>2</mn></msub><msub><mi>X</mi><mn>2</mn></msub><mo>+</mo><mo>‚Ä¶</mo><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mtext>exp</mtext><mspace width="2px"></mspace><mrow><mo>(</mo><mrow><msub><mi>Œ≤ÃÇ</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>2</mn></msub><msub><mi>X</mi><mn>2</mn></msub><mo>+</mo><mo>‚Ä¶</mo><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mrow></math><p>Alternatively classification trees can be used</p></div><hr><div class="container pre--container"><h3>Classification Trees</h3><ul class="no-item-padding" style="margin-block: 0px;"><li>Set of rules based on input variables to make predictions</li><li>The set of rules can be summarised in a tree</li><li>Simple intepretation</li><li>Closely resemble human decison making.</li><li>Useful for communciating predictions and other results ot non-techincal stakeholders</li><li>Trees are the based of many popular machine learning algorithms</li></ul><p>Each Node conatains predicted classificatoin &amp; probablilities</p></div><hr><div class="container pre--container"><h3>Evaluation classification models</h3><p>In the continious case we want small prediction errors</p><math display="block"><mrow><mtext>Accuracy Rate</mtext><mo>=</mo><mfrac><mtext>Correct Predictions</mtext><mtext>Number of Observations</mtext></mfrac></mrow></math><ul class="no-item-padding" style="margin-block: 0px;"><li>In classificaiotn problems are either right or wrong</li><li>Assess model accuracy in terms of accuracy rate after classification</li><li>Observe y_i = 1 or 0, predict index_i = P(y_i = 1 | x_i)</li><li>Default threshold of 0.5 used P_i to assign prediciton</li></ul><math display="block"><mrow><msub><mi>≈∑</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable><mtr><mtd columnalign="left"><mn>1</mn></mtd><mtd columnalign="left"><mtext>if</mtext></mtd><mtd columnalign="left"><mrow><msub><mi>PÃÇ</mi><mi>i</mi></msub><mo>&gt;</mo><mn>0.5</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mrow></mrow></mtd><mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math></div><hr><div class="container pre--container"><h3>Confusion Matrix</h3><table class="diagram-table"><thead><tr><th colspan="2" rowspan="2"></th><th colspan="2"><font color="blue">Predicted</font></th></tr><tr><th colspan="1">A</th><th colspan="1">B</th></tr></thead><tbody><tr><th rowspan="2"><font color="red">Actual</font></th><th>A</th><td>True Positive</td><td>False Negative</td></tr><tr><th>B</th><td>False Positive</td><td>True Negative</td></tr></tbody></table><p>
          Accuracy rate can be obtained from a 2x2 table
          called a confusion matrix.
        </p></div><hr><div class="container pre--container"><h3>Algorithm Ethics</h3><p>
          Due to a combination of automatic decision making and
          the black box nature of an algorithm sometimes, questions
          of accountablity are raised. It is entirely possible an
          algorithm could produce an outcome that disadvantage or
          priviliege a certain group unfairly.
        </p><h4>Identifying Biases</h4><ul class="no-item-padding" style="margin-block: 0px;"><li>Measure of Bias, difference in estimates when controlling for classification</li></ul><h4>Risk adjusted algorithms</h4><ul class="no-item-padding" style="margin-block: 0px;"><li>Use objective measures<ul class="no-item-padding" style="margin-block: 0px;"><li>Input X includes past clasims, diagnosis, medications, socio-demographics (without race)</li><li>Predicted risk does well in predicting future cost</li></ul></li></ul><h4>Questions</h4><dl><dt>Do solutions lie with improved algorithms?</dt><dd><div class="container pre--container"><ul class="no-item-padding" style="margin-block: 0px;"><li>not necessarily, sometimes its the training data</li><li>sometimes it can be, like the robodebt algo</li><li>Accountablity will typically require subject domain input</li></ul></div></dd><dt>Does bias in imply bias out?</dt><dd><div class="container pre--container"><ul class="no-item-padding" style="margin-block: 0px;"><li>A common belief is yes</li><li>Given existing biases might sem obviosu that the algorithm would replicate these</li><li>Need to be careful to consider sample used as training data</li><li>But it depends</li></ul></div></dd><dt>Should algorithm be blinded?</dt><dd><div class="container pre--container"><ul class="no-item-padding" style="margin-block: 0px;"><li>Seems fair, and sensible</li><li>But how do you define fair?</li><li>It depends</li></ul></div></dd><dt>Should algorithm be interpretable?</dt><dd><div class="container pre--container"><ul class="no-item-padding" style="margin-block: 0px;"><li>Interpretation is a valuable trait</li><li>But if there is a more correct algorithm is the lower accuracy justifible?</li><li>Maybe it is if there needs to be a level of transparency</li></ul></div></dd></dl></div></div></details><details name="week" class="dashbox"><summary><h2>Week 8 ‚Äî Research Design &amp; Experimentation, Part 1</h2></summary><div class="container pre--container"><div class="container pre--container"><h3>Motivation</h3><blockquote>Correlation does not imply causation</blockquote><ul class="no-item-padding" style="margin-block: 0px;"><li>Correlation measure strength of association between two variables</li></ul></div><div class="container pre--container"><h3>Introduction</h3><p>Experiments are one way to obtain causal effects</p><ul class="no-item-padding" style="margin-block: 0px;"><li>Field Experiments</li><li>A/B Experiments</li><li>Quasi experiments</li></ul></div><hr><div class="container pre--container"><h3>Causality &amp; notin of ceteris paribus</h3><blockquote>
          Causation is somethign that makes a difference, and
          the difference it makes must be a difference from what
          would have happened without it. ‚Äî David Lewis
        </blockquote><ul class="no-item-padding" style="margin-block: 0px;"><li>
            In evaluating an intervention (or policy change)
            think of couterfactual outcomes &amp; what if questions
          </li><li>
            In regression if you want to define a causal effect of x on y.
            How does y change if x is chagned but all other relevant factors
            are held constants?
          </li></ul><p>
          While multi regressions are one way to control for a other
          variables an even better way is run a randomised control trial (RCT).
          RCTs are suggested as the gold standard.
        </p></div><hr><div class="container pre--container"><h3>Conducting RCTs</h3><ol class="no-item-padding" style="margin-block: 0px;"><li>Decide on form of intervention and population of interest</li><li>Determine output of interest</li><li>Decide on randomisation unit</li><li>
            Determine sample size &amp; randomly assign units to
            treatment (new program/website) &amp; control (no program/old
            website/different website version) groups.
          </li><li>Compare outcomes to determimne treatment effect.</li><li>Findings are input into decision to adapt (implemtn program/use new website) or not.</li></ol></div><hr><div class="container pre--container"><h3>Experimental Evidence</h3><p>Experimental evidence on input into decision making</p><p>
          A significant treatement effect from RCT maybe not be
          enough to justify implementation of intervention.
        </p><ul class="no-item-padding" style="margin-block: 0px;"><li>Interventions are costly so size of any benefite needs to be weighted against cost</li><li>Does intervention represent value for money?</li><li>Does it have harmful unintended consequences?</li></ul><p><strong>Null results are useful!</strong></p><ul class="no-item-padding" style="margin-block: 0px;"><li>Finding no treatement effect could avoid unecessary costs</li></ul><p>Interventions may be intuitively appealing</p><ul class="no-item-padding" style="margin-block: 0px;"><li>But typically many intuitively appealing interventions may not work.</li><li>Knowing which is better &amp; wehther they are value for money, requires supporting evidence.</li></ul><p>once implemented evidecne shoud continue to be collected &amp; interventions refined</p><ul class="no-item-padding" style="margin-block: 0px;"><li>Interventions may work in one population but not another</li><li>
            Replication of core findings across experiments is more compelling evidence
            than a very significant effect in a single study
          </li></ul></div></div></details><details open="open" name="week" class="dashbox"><summary class="h2"><h2>Week 9 ‚Äî Research Design &amp; Experimentation, Part 2</h2></summary><div class="container pre--container"><div class="container pre--container"><h3>Challenge of Causal inference</h3><ul class="no-item-padding" style="margin-block: 0px;"><li>Direct experimentation may not be possibl, (e.g. oil spill), as it may not be ethical</li><li>For causal interpretation need all other factors to be held constant</li><li>Also need disturbance (eplison) to remain unchanged as the treatment changes</li><li>Data availability provides other opportunities to exploit quasi-experimental methods</li><li>Natural experiments where randomisation is accidental or not done as part of an experiment</li></ul></div><hr><div class="container pre--container"><h3>Hedonic Regressions</h3><math display="block"><mtable><mtr columnalign=""><mtd columnalign=""><mtd><munder><mrow><mrow><mtext>log</mtext><mspace width="2px"></mspace><mrow><mo>(</mo><msub><mi>≈∑</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><msub><mi>Œ≤ÃÇ</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>1</mn></msub><msub><mi>subject</mi><mi>i</mi></msub><mo>+</mo><msub><mi>Œ≤ÃÇ</mi><mn>2</mn></msub><msub><mi>event</mi><mi>i</mi></msub><mo>+</mo><mover><mrow><msub><mi>Œ≤ÃÇ</mi><mn>3</mn></msub><msub><mi>event</mi><mi>i</mi></msub><mo>¬∑</mo><msub><mi>subject</mi><mi>i</mi></msub></mrow><mover><mo>‚èû</mo><mrow><mtext>Difference in Difference</mtext></mrow></mover></mover><mo>+</mo><msub><mi>Œµ</mi><mi>i</mi></msub></mrow><mtext>subject is the opposite of control</mtext></munder></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mspace width="0px"></mspace></mtd></mtd></mtr><mtr columnalign=""><mtd columnalign=""><mtd><mrow><mtext>Classification</mtext><mo>=</mo><mrow><mo>{</mo><mtable><mtr><mtd columnalign="left"><mtext>"Subject After Event"</mtext></mtd><mtd columnalign="left"><mtext>if</mtext></mtd><mtd columnalign="left"><mrow><msub><mi>subject</mi><mi>i</mi></msub><mo>=</mo><msub><mi>event</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mtext>"Control After Event"</mtext></mtd><mtd columnalign="left"><mtext>if</mtext></mtd><mtd columnalign="left"><mrow><msub><mi>event</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mtext>"Subject Before Event"</mtext></mtd><mtd columnalign="left"><mtext>if</mtext></mtd><mtd columnalign="left"><mrow><msub><mi>subject</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mtext>"Control Before Event"</mtext></mtd><mtd columnalign="left"><mrow></mrow></mtd><mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></mtd></mtd></mtr></mtable></math><p>
          Say if you wanted to simulate the impact of an oil spill
          on house prices, you could take the above, replace y with
          house price, replace subject with "near" as in near the
          oil spill, and replace event with "after" as in after
          the spill and then you can measure the difference in
          difference.
        </p></div></div></details></div></div><hr style="width: 100%; border-color: transparent;">"
`;
