// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`app(unsw::2206::08.1).snapshot > should match snapshots 1`] = `
"<style id="test-styles"></style><div class="widget-container document" id="test-widget-0" style="grid-column: span 2;"><style id="test-widget-0-styles"></style><div class="container pre--container"><div class="c2 twoThree pre--c2 pre--two-three"><div class="container pre--container"><h1>Basic Regression Analysis with Time Series Data<br><small style="color: #aaaaff">ECON2206, W8, Lecture 1</small></h1></div><aside class="infobox"><h3 class="infobox-name">üí° Resources</h3><div class="infobox-body"><div class="container"><ul class="no-item-padding" style="margin-block: 0px;"><li>Chapter 10 of the textbook.</li><li><a href="https://www.tylervigen.com/spurious-correlations" target="_blank">Spurious regressions</a></li></ul></div></div></aside></div></div></div><div class="widget-container document" id="test-widget-1" style="grid-column: span 2;"><style id="test-widget-1-styles"></style><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h2>The Nature of Time Series Data</h2><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P334)</strong></span></div><div class="container dashbox" style="padding: 16px; border: var(--border-white-dash)"><div class="c2 twoThree pre--c2 pre--two-three"><div class="container pre--container"><p>
          Time series data places heavy emphasis on the fact
          it has a temporal ordering. While this fact should not
          be surspring it is worth emphasising as it is the lense
          in which we'll be revaluating the methodologies we've
          learnt so far. We'll still have how tests, our estimators,
          our statistics, etc. But this temporal ordering aspect
          introduces so idiosyncrasies.
        </p><hr><p>
          Something else worth considering is the whole concept of
          a random variable over time. How do we think about the
          concept of randomness over a time series? In cross
          sectional data we thought of this as a random draw,
          which isn't immediately obvious how it translates
          to what into time series data.
        </p><ul class="no-item-padding" style="margin-block: 0px;"><li>
            Generally, <strong>because the outcomes of our variables are
            not foreknown, they are viewed as random variables</strong>.
          </li><li>
            One set over observations collected over a time series,
            is one possible outcome, or realization, of the stochastic
            process.
          </li></ul></div><dl><dt>temporal ordering</dt><dd>Ordering over time</dd><dt>stochastic process</dt><dd>
          Sometimes called a <strong>times series process</strong>, this
          is a sequence of random variables indexed by time.
        </dd><dt>Stochastic</dt><dd>A synonym for random</dd></dl></div><h3>Summary, Compared with Cross Sectional</h3><div class="c2 pre--c2"><div class="container pre--container"><h4>Cross Sectional Data</h4><p>
          TODO. Observation are independent from each other.
          Until now most of the questions we've asked relating
          to our causation is what would have happened.
          Counterfactuals in cross sectional here have been
          like what would Y be if X‚±º was blah.
        </p></div><div class="container pre--container"><h4>Time series data</h4><p>
          TODO.
          Sample is one realised path. Due to serial correlation
          one draw is expected to be influenced by a previous draw.
          Counterfactuals in time series here are like, how would
          Y evolve given X.
        </p></div></div><p class="text-large" style="color: var(--fg-white-on-red); background: var(--bg-red); padding: 8px;"><strong>PLACEHOLDER</strong>: fix summary, clarify what is randomw and what are counter factorals</p></div></div></div><div class="widget-container document" id="test-widget-2" style="grid-column: span 2;"><style id="test-widget-2-styles"></style><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h2>Examples of Time Series Regression Models</h2><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P335)</strong></span></div><div class="container dashbox" style="padding: 16px; border: var(--border-white-dash)"><div class="c2 twoThree pre--c2 pre--two-three"><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h3>Static Models</h3><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P336)</strong></span></div><figure class="pre--horizontal-shadow"><math display="block"><mrow><msub><mi>y</mi><mi>t</mi></msub><mspace width="4px"></mspace><mo>=</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ≤</mi><mn>0</mn></msub><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ≤</mi><mn>1</mn></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>1</mn></mrow></msub></mrow><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mo>‚ãØ</mo><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ≤</mi><mi>k</mi></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mi>k</mi></mrow></msub></mrow><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><msub><mi>u</mi><mi>t</mi></msub></mrow></mrow></math><figcaption>Example of a static model</figcaption></figure><p>
          Above is an example of a static model, there is no
          reference to any other time period so any value of
          the explanatory variables are understood to have an
          immediate effect on the value of y.
        </p><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h3>Finite Distributed Lag Models</h3><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P336)</strong></span></div><figure class="pre--horizontal-shadow"><math display="block"><mrow><msub><mi>y</mi><mi>t</mi></msub><mspace width="4px"></mspace><mo>=</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ±</mi><mn>0</mn></msub><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ¥</mi><mn>0</mn></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>1</mn></mrow></msub></mrow><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ¥</mi><mn>1</mn></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><msub><mi>x</mi><mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>1</mn></mrow></msub></mrow><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ¥</mi><mn>2</mn></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><msub><mi>x</mi><mrow><mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>1</mn></mrow></msub></mrow><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><msub><mi>u</mi><mi>t</mi></msub></mrow></mrow></math><figcaption>Example of a static model</figcaption></figure><p>
          Above is an example of a lagged model, their explanatory
          variables can consist of variables from 1 or more earlier
          periods. The variables from the current period do not
          entirely determine the value of the current period and
          their effect isn't immediate.
        </p><p>
          The order of the models lag effect is the number of periods
          referenced before the current period.
        </p><ul class="no-item-padding" style="margin-block: 0px;"><li>
            <strong>FDL of order 2</strong> references at most 2 periods
            <ul class="no-item-padding" style="margin-block: 0px;"><li>The above example is an example of FDL 2</li><li>FDL of order 2 can reference the current, last, and last last period.</li><li>If it had a variable with a subscript of <strong>t-3</strong>, then it would be FDL of order 3</li></ul>
          </li></ul><p>The difference between a <strong>horizon</strong> and the <strong>LRP</strong>:</p><ul style="margin-block: 0px;"><li>
            <strong><strong>horizon</strong></strong>: This is the effect after a permanent <strong>1 unit change</strong>
            after h periods.
          </li><li>
            <strong><strong>LRP</strong></strong>: This is the effect after a permanent <strong>change of any value</strong>
            after h periods.
          </li></ul><blockquote>
          Several terms have been given explaination on the side.
        </blockquote><p>
          Often the is substantial correlation in z at different
          lags resulting in multicollinearity leading to
          difficulty in obtaining precise estimates of the
          different iterations of the Œ¥‚±º.
        </p><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h3>A Convention about the Time Index</h3><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P338)</strong></span></div><p>
          When models have lagged explanatory variables, confusion can
          arise concerning the treatment of initial observations. The
          convention followed here will be that these are the initial
          values in our sample, so that we can always start the time
          index at <strong>t = 1</strong>. In practise this isn't too
          important but it's worth being clear about this as we
          explain some examples below.
        </p></div><div class="container pre--container"><dl><dt>Static model</dt><dd>
            The name from the fact that it models a contemporaneous
            relationship between the dependent variable and its
            explanatory variables.
          </dd><dt>Lagged model</dt><dd>
            The explanatory model consists of dependent variables
            from one or more earlier time period.
          </dd></dl><aside class="infobox"><h3 class="infobox-name">üí° Example of static models</h3><div class="infobox-body"><div class="container"><ul class="no-item-padding" style="margin-block: 0px;"><li>The phillips curve</li></ul></div></div></aside><dl><dt>Œ¥‚ÇÄ Impact Multiplier</dt><dd>
            In a finite distributed lag model, the
            coefficient referring to the current
            period is the <strong>impact propensity</strong>
            or <strong>impact multiplier</strong>, given it
            is the immediate effect on the dependent
            variable.
          </dd><dt>Œ¥‚±º Lag distributor</dt><dd>
            <strong>Œ¥‚±º</strong> is a function of <strong>j</strong> which
            takes the period and returns the effect.
          </dd><dt>LRP</dt><dd>
            <strong>Long run propensity</strong> or long run multiplier, this
            is when there's no further changes in a variable
            with a <strong>Lag distributor</strong>. E.g.<br>
            <br>
            <strong>z(Œ¥‚ÇÄ + Œ¥‚ÇÅ + Œ¥‚ÇÇ)</strong>
          </dd><dt>Horizon (h)</dt><dd>
            A time of h periods.
          </dd><dt>Cummulative effect</dt><dd>
            For a horizon h, the cumulative
            effective effect is the change
            in the dependent variable after
            h periods after a permanent <strong>one-unit</strong>
            increase in x given:<br>
            <br>
            <strong>x(Œ¥‚ÇÄ + Œ¥‚ÇÅ + Œ¥‚ÇÇ)</strong>

          </dd></dl></div></div></div></div></div><div class="widget-container document" id="test-widget-3" style="grid-column: span 2;"><style id="test-widget-3-styles"></style><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h2>Finite Sample Properties of OLS under Classical Assumptions</h2><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P339)</strong></span></div><div class="container dashbox" style="padding: 16px; border: var(--border-white-dash)"><p>
      Here we'll go through the 6 assumptions
      needed to ensure unbiasness in OLS.
    </p><div class="c2 twoThree pre--c2 pre--two-three"><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h3>Unbiasedness of OLS</h3><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P339)</strong></span></div><p>Time series OLS is unbias under TS.1, TS.2 &amp; TS.3</p><h4>TS.1 Linear in Parameters</h4><div class="container pre--container"><figure class="pre--horizontal-shadow"><math style="--fontsize-math-m: 14px" display="block"><mtable><mtr columnalign="right center left"><mtd columnalign="right"><mrow><mo>{</mo><mrow><mo>(</mo><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>1</mn></mrow></msub><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>2</mn></mrow></msub><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mo>‚ãØ</mo><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mi>k</mi></mrow></msub><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><msub><mi>y</mi><mi>t</mi></msub><mo>)</mo></mrow><mo>:</mo><mspace width="8px"></mspace><mi>t</mi></mrow></mtd><mtd columnalign="center"><mo>=</mo></mtd><mtd columnalign="left"><mrow><mrow><mn>1</mn><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mn>2</mn><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mo>‚ãØ</mo><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mi>n</mi></mrow><mo>}</mo></mrow></mtd></mtr><mtr columnalign="right center left"><mtd columnalign="right"><msub><mi>y</mi><mi>t</mi></msub></mtd><mtd columnalign="center"><mo>=</mo></mtd><mtd columnalign="left"><mrow><msub><mi>Œ≤</mi><mn>0</mn></msub><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ≤</mi><mn>1</mn></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>1</mn></mrow></msub></mrow><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ≤</mi><mn>2</mn></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>2</mn></mrow></msub></mrow><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mo>‚ãØ</mo><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><mrow><msub><mi>Œ≤</mi><mi>k</mi></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mi>k</mi></mrow></msub></mrow><mspace width="4px"></mspace><mo>+</mo><mspace width="4px"></mspace><msub><mi>u</mi><mi>t</mi></msub></mrow></mtd></mtr></mtable></math><figcaption>TS1: Linear in parameters</figcaption></figure><p>
            This is more or less the same as MLR, but since we
            are specifying a linear model for time series data. We
            formalise it differently. For reference purpose:
          </p><ul class="no-item-padding" style="margin-block: 0px;"><li>Let <strong>X</strong> refer to the collection of all independent variables for all time periods.</li><li>Let <strong>x‚Çú</strong> refer to the set of independent variables in period <strong>t</strong>.</li></ul></div><div class="container pre--container"><h4>TS.2 No Perfect Collinearity</h4><p>This is also identical to the equliviant MLR assumption.</p></div><div class="container pre--container"><h4>TS.3 Zero Conditional Mean</h4><blockquote>Require our explanator variables to be <strong>strictly exogenous</strong>.</blockquote><figure class="pre--horizontal-shadow"><math display="block"><mtable><mtr columnalign="right right center left"><mtd columnalign="right"><mtext>Let</mtext></mtd><mtd columnalign="right"><mrow><mi>ùîº</mi><mspace width="2px"></mspace><mrow><mo>(</mo><mrow><msub><mi>u</mi><mi>t</mi></msub><mspace width="2px"></mspace><mo>|</mo><mspace width="2px"></mspace><mi>X</mi></mrow><mo>)</mo></mrow></mrow></mtd><mtd columnalign="center"><mo>=</mo></mtd><mtd columnalign="left"><mn>0</mn></mtd></mtr><mtr columnalign="right right center left"><mtd columnalign="right"><mtext>Where</mtext></mtd><mtd columnalign="right"><mi>t</mi></mtd><mtd columnalign="center"><mo>‚àà</mo></mtd><mtd columnalign="left"><mrow><mo>{</mo><mn>1</mn><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mn>2</mn><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mo>‚Ä¶</mo><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mi>n</mi><mo>}</mo></mrow></mtd></mtr></mtable></math><figcaption>TS.3: Strictly exogenous</figcaption></figure><p>
            This is also similar to the equliviant MLR assumption, but slightly
            different.
          </p><ul style="margin-block: 0px;"><li>
              <strong>MLR.4</strong>: for observation i, u<sub>i</sub>
              should not correlate with an explanatory variable
              from that obsevation.
            </li><li>
              <strong>TS.3</strong>: for period t, u<sub>t</sub> should
              not correlate an explanatory variable,
              <strong>from any period</strong>.
            </li></ul><p>
            Generally explanatory variables need to be strictly exogenous so
            they cannot react to what has happened to y in the past (technically
            also the future but in practise this isn't something we need to be
            worried about unless we make some weird assumptions we can't even
            measure).
          </p><p>
            <strong>The reason for this difference</strong> is the fact is
            MLR assumes all observation are independent as result of based
            random sampling, our observations are absolutely not independent
            and almost always dependent on each other. Due to the very nature
            of times series we can not address this correlation problem via
            independence as a result of random sampling, due to the time nature
            of our observations our data can almost never be independent. So
            this shifts the responsibility of addressing this independence
            problem into our zero conditional mean assumption.
            <strong>Hence we need strict exogenity</strong>. When we only do it at
            the period level we call this <strong>Contemporaneously Exogenous</strong>.
          </p><figure class="pre--horizontal-shadow"><math display="block"><mrow><mrow><mi>ùîº</mi><mspace width="2px"></mspace><mrow><mo>(</mo><mrow><msub><mi>u</mi><mi>t</mi></msub><mspace width="2px"></mspace><mo>|</mo><mspace width="2px"></mspace><mrow><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mn>1</mn></mrow></msub><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mo>‚Ä¶</mo><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mi>k</mi></mrow></msub></mrow></mrow><mo>)</mo></mrow></mrow><mspace width="4px"></mspace><mo>=</mo><mspace width="4px"></mspace><mrow><mi>ùîº</mi><mspace width="2px"></mspace><mrow><mo>(</mo><mrow><msub><mi>u</mi><mi>t</mi></msub><mspace width="2px"></mspace><mo>|</mo><mspace width="2px"></mspace><msub><mi>x</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mspace width="4px"></mspace><mo>=</mo><mspace width="4px"></mspace><mn>0</mn></mrow></math><figcaption>Contemporaneously exogenous</figcaption></figure><p>However there are some similarities:</p><ul class="no-item-padding" style="margin-block: 0px;"><li>This requires us to specify the correct functional form.</li></ul><aside class="infobox"><h3 class="infobox-name">üí° u correlated with x‚±º</h3><div class="infobox-body"><div class="container"><p>In practise:</p><ul class="no-item-padding" style="margin-block: 0px;"><li>Present <strong>u‚Çú</strong> will not be correlated with past <strong>x‚±º‚Çú</strong></li><li>Present <strong>u‚Çú</strong> will almost correlate with future <strong>x‚±º‚Çú</strong></li></ul></div></div></aside><blockquote>
            <strong>TS.3</strong> is fairly unrealistic in practise, but we need to
            state it and be mindful of it and take steps to minimise
            violations of it.
            In the minimisation of coefficents in the OLS objective function
            the mathetmatical constraint assumes there is no correlation (it
            would be impossible to do otherwise as we have not observed the
            value), so while we might accept it isn't the case the mathematics
            does not so we need to be taking steps to minimise it (if not remedy
            it) where possible.
          </blockquote></div></div><div class="container pre--container"><aside class="infobox"><h3 class="infobox-name">üí° TS Assumptions</h3><div class="infobox-body"><div class="container"><ol class="no-item-padding" style="margin-block: 0px;"><li>Linear in Parameters</li><li>No Perfect Collinearity</li><li>Zero Conditional Mean</li><li>Homoskedasticity</li><li>No Serial Correlation</li><li>Normality</li></ol></div></div></aside><aside class="infobox"><h3 class="infobox-name">üí° Assumptions for unbiasness</h3><div class="infobox-body"><div class="container"><ol class="no-item-padding" style="margin-block: 0px;"><li>Linear in Parameters</li><li>No Perfect Collinearity</li><li>Zero Conditional Mean</li></ol></div></div></aside><dl><dt>Contemporaneously exogenous</dt><dd>When TS.3 holds at an observation level we call our explanatory variables this.</dd><dt>Strictly exogenous</dt><dd>When TS.3 holds generally we call our explanatory variables this.</dd><dt>Serial Correlation</dt><dd>
            This is also called <strong>Auto-Correlation</strong>, it is
            when our errors are correlated across time.
          </dd></dl><aside class="infobox"><h3 class="infobox-name">üí° Theorum: unbiasness of OLS</h3><div class="infobox-body"><div class="container"><p>
            Under TS.1-3, the OLS estimators are unbiased
            conditional on <strong>X</strong> and therefore
            unconditionally as well as when:<br>
            <br>
            <strong>E(Œ≤ÃÇ‚±º) = Œ≤‚±º, j ‚àà {0, 1, ..., k}</strong>
          </p></div></div></aside><aside class="infobox"><h3 class="infobox-name">üí° Theorum: Unbiased Esimation of œÉ¬≤</h3><div class="infobox-body"><div class="container"><p>
            Under TS.1-5, the estimator œÉÃÇ¬≤ = SSR/df is an
            unbiased estimator of œÉ¬≤ where:<br>
            <strong>df = n - k - 1</strong>.
          </p><h4>In FDL</h4><p>
            In an FDL model, lag distributors are highly
            likely to be correlated because it isn't
            uncommon for them to be the same value alot
            of the time.
          </p></div></div></aside><aside class="infobox"><h3 class="infobox-name">üí° Theorum: Guass-Markov Theorum</h3><div class="infobox-body"><div class="container"><p>
            Under TS.1-5, OLS estimators are the best linear
            unbiased estimators on <strong>X</strong>.
          </p></div></div></aside><aside class="infobox"><h3 class="infobox-name">üí° Normal Sampling Distribution</h3><div class="infobox-body"><div class="container"><p>
            <strong>Theorum</strong>: Under TS.1-6, the CLM assumptions for time series, the OLS
            estimators are normally distributed, conditional on X.
            Usage contruction of t &amp; F statistics are valid and have
            their respective distribution (t &amp; F distributions). CI's
            are also valid.
          </p></div></div></aside></div></div><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h3>The Variances of the OLS Estimators and the Gauss-Markov Theorem</h3><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P342)</strong></span></div><div class="container pre--container"><h4>TS.4 Homoskedasticity</h4><blockquote>Variance of unobservables is constant over time</blockquote><div class="c2 pre--c2"><div class="container pre--container"><p>
              This is similar to MLR.5, but like TS.3 being
              similar to MLR.4. Really what we are saying is
              the variance of our observable are constant over
              time.
            </p><p>
              This can be false sometimes when there's some kind
              of instutional reform of regulatory change which
              affect variance of our explanatory variables.
            </p></div><figure class="pre--horizontal-shadow"><math display="block"><mtable><mtr columnalign="right right center left"><mtd columnalign="right"><mtext>Let</mtext></mtd><mtd columnalign="right"><mrow><mi>ùïç</mi><mspace width="2px"></mspace><mrow><mo>(</mo><mrow><msub><mi>u</mi><mi>t</mi></msub><mspace width="2px"></mspace><mo>|</mo><mspace width="2px"></mspace><mi>X</mi></mrow><mo>)</mo></mrow></mrow></mtd><mtd columnalign="center"><mo>=</mo></mtd><mtd columnalign="left"><mrow><mrow><mi>ùïç</mi><mspace width="2px"></mspace><mrow><mo>(</mo><msub><mi>u</mi><mi>t</mi></msub><mo>)</mo></mrow></mrow><mspace width="4px"></mspace><mo>=</mo><mspace width="4px"></mspace><msup><mi>œÉ</mi><mn>2</mn></msup></mrow></mtd></mtr><mtr columnalign="right right center left"><mtd columnalign="right"><mtext>Where</mtext></mtd><mtd columnalign="right"><mi>t</mi></mtd><mtd columnalign="center"><mo>‚àà</mo></mtd><mtd columnalign="left"><mrow><mo>{</mo><mn>1</mn><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mn>2</mn><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mo>‚Ä¶</mo><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mi>n</mi><mo>}</mo></mrow></mtd></mtr></mtable></math><figcaption>TS.4: Homoskedasticity</figcaption></figure></div><p>
          We can reuse the heteroskedascity tests from eariler
          chapters.
        </p></div><div class="container pre--container"><h4>TS.5 No Serial Correlation</h4><div class="c2 pre--c2"><div class="container pre--container"><p>
              This is largely saying that our unobservable are
              not correlated on previous unobservables. The
              reason this isn't explictly stated as an assumption
              for MLR is because under random sampling there's
              no reason to really expect it as its only a problem
              with time series data.
            </p><p>
              When this is false we say our errors suffer from
              <strong>serial correlation</strong>, it means our errors
              are correlated across time.
            </p></div><figure class="pre--horizontal-shadow"><math display="block"><mtable><mtr columnalign="right right left"><mtd columnalign="right"><mtext>Let</mtext></mtd><mtd columnalign="right"><mrow><mi>Corr</mi><mspace width="2px"></mspace><mrow><mo>(</mo><msub><mi>u</mi><mi>t</mi></msub><mo>,</mo><mspace width="4px"></mspace><mrow><msub><mi>u</mi><mi>s</mi></msub><mspace width="2px"></mspace><mo>|</mo><mspace width="2px"></mspace><mi>X</mi></mrow><mo>)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mo>=</mo><mspace width="4px"></mspace><mn>0</mn></mrow></mtd></mtr><mtr columnalign="right right left"><mtd columnalign="right"><mtext>Where</mtext></mtd><mtd columnalign="right"><mrow><mo>‚àÄ</mo><mi>t</mi><mo>.</mo><mspace width="8px"></mspace><mi>t</mi></mrow></mtd><mtd columnalign="left"><mrow><mo>‚â†</mo><mi>s</mi></mrow></mtd></mtr></mtable></math><figcaption>TS.4: Homoskedasticity</figcaption></figure></div><hr><div class="c2 pre--c2"><div class="container pre--container"><p>
              The equation on the right is how we assume variance in time
              series data, its actually the same theorum we use for cross
              sectional data. So no need to examine it to spot the difference
              its the same. But it warrants repeating.
            </p></div><figure class="pre--horizontal-shadow"><math display="block"><mtable><mtr columnalign="right right center left"><mtd columnalign="right"><mtext>Let</mtext></mtd><mtd columnalign="right"><mrow><mi>ùïç</mi><mspace width="2px"></mspace><mrow><mo>(</mo><mrow><msub><mi>Œ≤ÃÇ</mi><mi>j</mi></msub><mspace width="2px"></mspace><mo>|</mo><mspace width="2px"></mspace><mi>X</mi></mrow><mo>)</mo></mrow></mrow></mtd><mtd columnalign="center"><mo>=</mo></mtd><mtd columnalign="left"><mfrac><msup><mi>œÉ</mi><mn>2</mn></msup><mrow><mo>[</mo><msub><mi>SST</mi><mi>j</mi></msub><mspace width="1px"></mspace><mo>‚Å¢</mo><mspace width="1px"></mspace><mrow><mo>(</mo><mn>1</mn><mspace width="4px"></mspace><mo>-</mo><mspace width="4px"></mspace><msubsup><mi>R</mi><mi>j</mi><mn>2</mn></msubsup><mo>)</mo></mrow><mo>]</mo></mrow></mfrac></mtd></mtr><mtr columnalign="right right center left"><mtd columnalign="right"><mtext>Where</mtext></mtd><mtd columnalign="right"><mi>j</mi></mtd><mtd columnalign="center"><mo>‚àà</mo></mtd><mtd columnalign="left"><mrow><mo>{</mo><mn>1</mn><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mo>‚Ä¶</mo><mspace width="0px"></mspace><mrow><mo>,</mo><mspace width="2px"></mspace></mrow><mspace width="0px"></mspace><mi>k</mi><mo>}</mo></mrow></mtd></mtr></mtable></math><figcaption>OLS Sampling Variance</figcaption></figure></div></div></div><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h3>Inference under the Classical Linear Model Assumptions</h3><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P344)</strong></span></div><div class="c2 twoThree pre--c2 pre--two-three"><p>
          Assumption TS.6 implies TS.3-5, and it is the strongest assumption.
          It implies independence and normality, our standar errors,
          t statisitics, F statistics, etc. Operate as they did under
          MLR. However the classical linear model assumptions for time series
          data are much more restrictive than those for cross-sectional data.
        </p><aside class="infobox"><h3 class="infobox-name">üí° TS.6 Normality</h3><div class="infobox-body"><div class="container"><p>
            The errors u‚Çú are independent of X and are independently and
            identically distributed as <strong>Normal(0, œÉ¬≤)</strong>.
          </p></div></div></aside></div></div></div></div></div><div class="widget-container document" id="test-widget-4" style="grid-column: span 2;"><style id="test-widget-4-styles"></style><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h2>Functional Form, Dummy Variables, and Index Numbers</h2><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P345)</strong></span></div><div class="container dashbox" style="padding: 16px; border: var(--border-white-dash)"><div class="c2 twoThree pre--c2 pre--two-three"><div class="container pre--container"><p>
          All of what we learnt in terms of functional forms in MLR apply
          for time series regressions. However alot of emphasis is placed
          on the value on the log of either the dependent variable or
          the explantory variables.
        </p><h4>dummy variables</h4><p>
          These are useful for event indicators.
        </p></div><div class="container pre--container"><dl><dt>short-run elasticity</dt><dd>
            In a log log FDL model this is just
            the <strong>immediate propensity</strong> Œ¥‚ÇÄ.
          </dd><dt>long-run elasticity</dt><dd>
            In a log log FDL model this is just
            the <strong>long run propensity</strong>.
          </dd></dl></div></div></div></div></div><div class="widget-container document" id="test-widget-5" style="grid-column: span 2;"><style id="test-widget-5-styles"></style><div class="container pre--container"><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h2>Trends and Seasonality</h2><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P351)</strong></span></div><div class="container dashbox" style="padding: 16px; border: var(--border-white-dash)"><div class="c2 twoThree pre--c2 pre--two-three"><div class="container pre--container"><p style="
      color: var(--fg-white-on-blue);
      background: var(--bg-blue);
      padding: 8px
    "><p><strong>NOTE</strong><ul class="no-item-padding" style="margin-block: 0px;"></ul></p></p><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h4>Characterizing Trending Time Series</h4><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P351)</strong></span></div><p style="
      color: var(--fg-white-on-blue);
      background: var(--bg-blue);
      padding: 8px
    "><p><strong>NOTE</strong><ul class="no-item-padding" style="margin-block: 0px;"></ul></p></p><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h4>Using Trending Variables in Regression Analysis</h4><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P354)</strong></span></div><p style="
      color: var(--fg-white-on-blue);
      background: var(--bg-blue);
      padding: 8px
    "><p><strong>NOTE</strong><ul class="no-item-padding" style="margin-block: 0px;"></ul></p></p><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h4>A Detrending Interpretation of Regressions with a Time Trend</h4><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P356)</strong></span></div><p style="
      color: var(--fg-white-on-blue);
      background: var(--bg-blue);
      padding: 8px
    "><p><strong>NOTE</strong><ul class="no-item-padding" style="margin-block: 0px;"></ul></p></p><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h4>Computing R-Squared When the Dependent Variable Is Trending</h4><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P357)</strong></span></div><p style="
      color: var(--fg-white-on-blue);
      background: var(--bg-blue);
      padding: 8px
    "><p><strong>NOTE</strong><ul class="no-item-padding" style="margin-block: 0px;"></ul></p></p><div style="
      display: grid;
      grid-template-columns: 1fr auto;
      grid-gap: 8px;
    "><h4>Seasonality</h4><span style="
        display: grid;
        grid-auto-flow: column;
        grid-gap: 0.5em;
        align-items: center;
        max-height: 2em;
        line-height: 1;
        font-size: 10px;
        color: var(--fg-black-on-pink);
        background: var(--bg-pink);
        padding: 4px;
      "><strong>Book (C10 P358)</strong></span></div><p style="
      color: var(--fg-white-on-blue);
      background: var(--bg-blue);
      padding: 8px
    "><p><strong>NOTE</strong><ul class="no-item-padding" style="margin-block: 0px;"></ul></p></p></div><dl><dt>Time Trend</dt><dd>The tendency for data to follow a directional trend over time.</dd><dt>Exponential trend</dt><dd>A trend where a series grows protional to previous periods.</dd><dt>CAGR</dt><dd>constant average growth rate</dd><dt>growth rate</dt><dd>
          <figure class="pre--horizontal-shadow"><math display="block"><mrow><mrow><mi>ùõ•</mi><msub><mi>y</mi><mi>t</mi></msub></mrow><mspace width="4px"></mspace><mo>‚âà</mo><mspace width="4px"></mspace><mfrac><mrow><msub><mi>y</mi><mi>t</mi></msub><mspace width="4px"></mspace><mo>-</mo><mspace width="4px"></mspace><msub><mi>y</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><msub><mi>y</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mfrac></mrow></math><figcaption>Growth Rate</figcaption></figure>
        </dd><dt>spurious regression problem</dt><dd>
          The challenge of finding a relationship between
          two or more trending variables simply because
          each is growing over time.
        </dd><dt>detrending</dt><dd>
          The removal of a trend from a dataset.
        </dd><dt>Linear Detrended</dt><dd>
          Often annontated with <strong>Umlaut</strong> like so <strong>yÃà</strong>.
        </dd><dt>Seasonality</dt><dd>Exhibting a trend over a period</dd><dt>Seasonally adjusted</dt><dd>The removal of seasonality</dd></dl></div></div></div></div><hr style="width: 100%; border-color: transparent;">"
`;
