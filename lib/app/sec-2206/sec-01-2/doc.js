/**
 * @import { E, DocumentWidget, Widget } from '../../prelude-type.ts';
 * @import { Config, State } from './type.ts';
 */
import { doc, table } from '../../prelude.js';
import * as prelude from '../prelude.js';
import * as mathml from './mathml.js';
import * as tables from './tables.js';

const {
  twoColumns, twoThree, container,
  responsiveGridAutoRow,
} = prelude.layout;
const { text, infobox, dashbox, note, todo } = prelude.components;
const { createDoc } = prelude.util;


const LINKS = {
  /** @type {E.Item} */
  linearRegressionInStata: ['a', {
    href: 'https://www.princeton.edu/~otorres/Regression101.pdf',
  }, ['Linear Regression in Stata']],
};

const TABLES = {
  terminology: table(['Y', 'X'], [
    ['Dependent variable', 'Independent variable'],
    ['Explained variable', 'Explanatory variable'],
    ['Response variable', 'Control variable'],
    ['Predicted variable', 'Predictor variable'],
    ['Regressand variable', 'Regressor variable'],
  ]),
  logFunctionalForms: twoColumns(
    table(['Model', 'Dependent Var'], [
      ['Level-Level', 'y'],
      ['Level-Log', 'y'],
      ['Log-Level', 'log(y)'],
      ['Log-Log', 'log(y)'],
    ]),
    table(['Independent Var', 'Interpreation of Œ≤ÃÇ‚ÇÅ'], [
      ['x', 'Œîy = Œ≤‚ÇÅ Œîx'],
      ['log(x)', 'Œîy = (Œ≤‚ÇÅ/100) %Œîx'],
      ['x', '%Œîy = (100Œ≤‚ÇÅ) Œîx'],
      ['log(x)', '%Œîy = Œ≤‚ÇÅ %Œîx'],
    ])
  ),
};

const INFOBOX = {
  populationParamaters: infobox('Population Parameters', [container(
    doc.p`
      Œ≤‚ÇÄ and Œ≤‚ÇÅ of the population are referred to as
      population parameters. They are unknown in
      reality.
    `,
  )]),
  noteOnIndexSyntax: infobox('Note on Index Syntax', [container(
    doc.p`
      While subscript 1 & 0 are used for the coefficents
      of a linear regression, when used on variables it is
      in reference to an observation in a sample. It is
      often generalised as an i, as in ${doc.i`x·µ¢`}
      for the x of the postion i where i is a number between
      1 and n (sometimes this can be 0 & n-1).
    `,
    mathml.indexSyntax,
  )]),
  propertiesOfSummation: infobox('Properties of Summation', [container(
    todo({}, 'Write me later'),
  )]),
  betaOneIsARandomVariable: infobox('Œ≤ÃÇ‚ÇÅ is a random variable', [container(
    doc.p`
      Œ≤ÃÇ·µ¢ is a random variable, in that each time you
      generate a sample you draw a new value
    `,
  )]),
};

/**
 * @param {string} label
 * @returns {Widget<any, State, Config>}
 */
export const createPlaceholder = (label) => (
  createDoc(() => container(
    todo({}, `for "${label.trim()}"`),
  ))
);

export const intro = createDoc(() => container(
  twoThree(
    container(
      ['h1', [
        'Simple Regression Model', ['br'],
        ['small', { style: 'color: #aaaaff' }, ['ECON2206, W1, Lecture 2']],
      ]],
      doc.p`
        To start with analysing cross sectional data we are
        going to look at simple regression models.
      `,
    ),
    infobox('Resources', [container(
      text.ul.compact.of(
        doc.li`See chapter 2 of the textbook.`,
        doc.li.of(LINKS.linearRegressionInStata),
      ),
    )]),
  ),
));

export const models = createDoc(() => dashbox(
  doc.h2`Modelling and understanding problems`,
  twoColumns(
    container(
      doc.h3`Theoretical Model`,
      doc.p`
        When tackling economic problems or understanding the
        economic nature of a system (an economy, institution,
        industry, etc), or economic characteristic of a system
        and its partipants, it helps to start with an theoretical
        model in which modifications to the system can be thought
        through and quantitied.
      `,
      doc.p`
        In a way its an attempt to explain the nature of the economic
        system in terms of the thing in of itself, and it can be
        expressed in terms of properties that observable and unobservable.
        The problem of measuring these things will come later.
      `,
    ),
    container(
      doc.h3`Economic Models`,
      doc.p`
        This can also be thought of an ${doc.b`Econometric model`}
        and its when we start to concern ourselves with what we can
        and cannot measure. The goal is to map theory to what we can
        observe, and vice versa. Eventually to see where reality relates
        to these models, or if our models are actually representive of
        reality.
      `,
      doc.p`
        Given the focus on what we can measure, we only include
        what we can measure within the model. In a simple
        linear regression model, this is captured in the error term,
        so there are at least some way of accounting for these in
        relative terms to what we can observe. We call these ${
        doc.b`unobservable variables`}.
      `,
    ),
  ),
));

export const linearRegression = createDoc(() => dashbox(
  doc.h2`Linear Regression`,
  doc.h3`Terminology`,
  twoThree(
    container(
      doc.figure(mathml.simpleLinearRegresion, 'Simple Linear regression'),
      doc.p`${doc.b`u`}
        is sometimes called the erro term of distubance in
        the relationship, representing factors other than
        x that affect y. You can also simply think of it as ${
        doc.b`unobserved`}.
      `,
    ),
    TABLES.terminology,
  ),
  doc.h3`Population Assumptions and Definitions`,
  twoColumns(
    container(
      doc.h4`Linear Effect of X on Y`,
      doc.figure(mathml.xyFunctionalRelationship, 'Linear effect requirement'),
      doc.p`
        Given the above is derived from the regression with Œ≤‚ÇÄ, we should
        see a zero change in u with an increase in x, this also means Œ≤‚ÇÅ
        is the ${doc.b`slope`} of the relationship between x and y.
      `,
    ),
    container(
      doc.h4`Average value of u`,
      doc.figure(mathml.assumeExpectedUToBe0, 'Average u in the population given Œ≤‚ÇÄ'),
      doc.p`
        One assumption we need to make before we assume how x and u
        relate to one another is that for our model the average u
        in the population should be zero.
      `,
    ),
  ),
  container(
    doc.h4`Average U does not depent on X`,
    doc.figure(mathml.assumeExpectedUDoesNotDependOnX, 'Assume U does not depend on X'),
    doc.p`
      This says that the average ${doc.b`u`} across the
      population determined by x is equal to the average ${doc.b`u`}
      over the population. Put another way, this says that u is ${
      doc.b`mean independent`} of x. Combining this with E(u) = 0,
      we get the zero conditional mean assumption.
    `,
    doc.figure(mathml.assumeZeroConditionalMean, 'Zero Conditional mean assumption'),
  ),
  ['hr'],
  container(
    doc.h4`Population regression function (PRF)`,
    responsiveGridAutoRow([
      doc.figure(mathml.populationRegressionFunction, 'The PRF'),
      doc.figure(mathml.populationRegressionFunctionSimple, 'Also the PRF'),
      INFOBOX.populationParamaters,
    ], {
      columns: {
        desktop: '2fr 2fr 3fr',
      },
    }),
    doc.p`
      The right hand side does not mean, y = [right hand side]. It is
      more accurate to read it as a one input increase in x changes the
      expected value by the amount of Œ≤‚ÇÅ. So if you collected all observations
      of Y and X where Y was the same value, you'd see a normal distribution
      for all the values of X where the average value was (Y - Œ≤‚ÇÄ) √∑ Œ≤‚ÇÅ.
      Given this we can break the regression down into 2 components:
    `,
    text.ul.compact.of(
      doc.li`${doc.b`Systematic part of y`}: E(y|x) or Œ≤‚ÇÄ + Œ≤‚ÇÅx`,
      doc.li`${doc.b`Unsystematic part of y`}: u, or the part y not explained by x`,
    ),
    doc.p`In reality we will never know the actual PRF in most meaningful cases.`,
  ),
));

export const ordinaryLeastSquaresEstimate = createDoc(() => dashbox(
  doc.h2`Deriving the Ordinary Least Squares Estimates`,
  twoThree(
    container(
      doc.p`
        In order to estimate the Œ≤‚ÇÄ and Œ≤‚ÇÅ for the population we
        need to sample from the population which we can produce
        our estimates as Œ≤ÃÇ‚ÇÄ and Œ≤ÃÇ‚ÇÅ. Given a sample of size ${
        doc.b`n`} each the X, Y & U for each observation is
        specified in the linear regression like so.
      `,
      doc.figure(mathml.simpleLinearRegresionSample, 'Sample linear regression'),
      doc.p`
        For each observation u·µ¢ is the information explaining
        y·µ¢ not captured in x·µ¢. It is also called the ${
        doc.b`error term`}.
      `,
      doc.figure(container(
        doc.p`${mathml.jointProbabilityDistributionOfXAndYInPop.p2_12}`,
        doc.p`${mathml.jointProbabilityDistributionOfXAndYInPop.p2_13}`,
      ), doc.span`Joint Probability distribution of x & y with Œ≤‚ÇÄ and Œ≤‚ÇÅ`),
      doc.p`
        It is helpful to think of the above of contraints in which we
        can count on solving for the values of Œ≤‚ÇÄ and Œ≤‚ÇÅ for the population.
        However in order to get estimates of the population from a sample
        we use ${doc.b`Sum of first order conditions`}.
      `,
    ),
    container(
      INFOBOX.noteOnIndexSyntax,
      doc.h4`Platonic properties of unobserved`,
      doc.figure(mathml.idealProperitesOfUnobserved.p2_10, 'mean unobserved is zero'),
      doc.figure(mathml.idealProperitesOfUnobserved.p2_11, 'covariance between u & x is zero'),
    ),
  ),
  doc.h4`Sum of First order condtiions`,
  doc.p`
    Using a method of moment approach we take the above population
    Joint distribution of x and y, assuming the properties of
    unobservables inrespect to x we derive these estimates allowing
    us to constrain our estimations for Œ≤‚ÇÄ and Œ≤‚ÇÅ.
  `,
  doc.figure(twoColumns(
    mathml.sumOfFirstOrderConditions.a,
    mathml.sumOfFirstOrderConditions.b,
  ), 'Sample counterparts to population above joint probability constraints'),
  twoThree(
    container(
      doc.h4`Estmating Œ≤ÃÇ‚ÇÄ`,
      doc.p`
        While we still need to know the value of Œ≤ÃÇ‚ÇÅ before we can solve
        for Œ≤ÃÇ‚ÇÄ, using the properties summation we can rewrite the above
        to get the following to estimate Œ≤ÃÇ‚ÇÄ.
      `,
      doc.figure(twoColumns(
        mathml.sumOfFirstOrderConditions.rewrite,
        mathml.sumOfFirstOrderConditions.betaZeroEstimate,
      ), 'Solving for Œ≤ÃÇ‚ÇÄ using properties of summation'),
    ),
    INFOBOX.propertiesOfSummation,
  ),
  container(
    doc.h4`Estmating Œ≤ÃÇ‚ÇÅ`,
    twoThree(
      doc.p`
        And continuing on from below we'll solve for Œ≤‚ÇÅ. We start by
        injecting our above definition of ${doc.b`Œ≤ÃÇ‚ÇÄ`} into the ${
        doc.b`Sum of first order conditions`}, and from there we'll
        seek to isolate Œ≤ÃÇ‚ÇÅ. The right hand equation (or subsequent) show
        the factoring undertaken before rearrive at our equation for Œ≤ÃÇ‚ÇÅ.
      `,
      INFOBOX.betaOneIsARandomVariable,
    ),
    twoColumns(
      doc.figure(mathml.betaOneEstimation.rearrangingAbove, doc.p`
        Start by plugging Œ≤ÃÇ‚ÇÄ into${['br']}
        the Sum of 1st Order Condition
      `),
      doc.figure(mathml.betaOneEstimation.alsoNotingThat, doc.p`
        Before isolating for Œ≤ÃÇ‚ÇÅ note${['br']}
        how these can be rearranged
      `),
    ),
    doc.p`
      Given the sum of (x${['sub', 'i']}-xÃÑ)¬≤ is greater than zero we
      compute Œ≤ÃÇ‚ÇÅ as shown, which can be further simplifed to be ùúéÃÇ${
      ['sub', 'y']} divided by ùúéÃÇ${['sub', 'x']} times ùúåÃÇ${
      ['sub', 'xy']} which is the correlation (Noting that ùúéÃÇ is the
      sample standard deviation and ùúåÃÇ is the sample correlation).
    `,
    responsiveGridAutoRow([
      doc.figure(mathml.betaOneEstimation.assumption, 'Given'),
      doc.figure(mathml.betaOneEstimation.betaOneOLS, 'Estimating Œ≤ÃÇ‚ÇÅ, the OLS slope'),
      doc.figure(mathml.betaOneEstimation.betaOneOLSSimplied, 'Simpified Œ≤ÃÇ‚ÇÅ'),
      doc.figure(mathml.betaOneEstimation.populationBetaOneOLSSimplied, 'Population Œ≤‚ÇÅ'),
    ], {
      columns: {
        desktop: 'auto auto auto auto',
      },
    }),
  ),
  container(
    doc.h4`Sample Regression Function, Fitted values and residuals`,
    doc.p`
      Using Œ≤ÃÇ‚ÇÄ with Œ≤ÃÇ‚ÇÅ times x·µ¢ we can get yÃÇ·µ¢ which is the
      ${doc.b`fitted value`} of y, and the difference between
      the observed y·µ¢ and its fitted value yÃÇ·µ¢ is called the
      ${doc.b`residual`} as uÃÇ·µ¢. Note this is actually different
      from the error term or unobserved.
    `,
    responsiveGridAutoRow([
      doc.figure(mathml.fittedValueOfYi, 'Fitted value of yÃÇ·µ¢'),
      doc.figure(mathml.fittedValueResiduals, 'Computing residual uÃÇ·µ¢'),
      doc.figure(mathml.sampleRegressionFunction, 'Sample Regression Function'),
    ], {
      columns: {
        desktop: '1fr auto 1fr',
      },
    }),
    doc.p`
      We've estimated our coefficents Œ≤ÃÇ‚ÇÄ and Œ≤ÃÇ‚ÇÅ so that the average uÃÇ·µ¢
      is 0, and you can see that if you start back at the Sum of
      First Order conditions where the right hand side is 0 and the
      residuals are omitted from the left hand side. At least that's
      how I understand it (there may be more formal ways of
      demostrating that).
    `,
  ),
  container(
    doc.h4`SST, SSE, SSR and Goodness of Fit`,
    doc.p`
      To measure ${doc.b`goodness of fit`} we'll first define
      a few more concepts which we will use to compute ${doc.b`R¬≤`}
      which is a way for assessing the overall fit of the data. They
      have been defined down below:
    `,
    responsiveGridAutoRow([
      doc.figure(mathml.sumOfSquares.sst, 'Total Sum of Squares'),
      doc.figure(mathml.sumOfSquares.sse, 'Explained Sum of Squares'),
      doc.figure(mathml.sumOfSquares.ssr, 'Residual Sum of Squares'),
    ], {
      columns: {
        desktop: 'auto auto auto',
      },
    }),
    doc.p`
      They all related to each other like as shown on the left, and
      you can use them like so compute the R-Squared which is sometimes
      called the Coefficient of determination. It is a measure of the
      portion of variance captured by the model.
    `,
    responsiveGridAutoRow([
      doc.figure(mathml.sumOfSquares.relation, 'Sum of Squares constraint'),
      doc.figure(mathml.rSquared, 'R Squared'),
    ], {
      columns: {
        desktop: 'auto auto',
      },
    }),
    doc.figure(mathml.rSquareFull, 'R Square Expanded'),
  ),
  container(
    doc.h4`What is Ordinary Least Squares really?`,
    doc.p`
      To be honest, I am still wrapping my head around
      this, but for the best answer I can give is, it
      solves for Œ≤ÃÇ‚ÇÄ and Œ≤ÃÇ‚ÇÅ so that you have the smallest
      value possible.
    `,
    doc.figure(
      mathml.whatIsOrdinaryLeastSquares,
      'Minimisation constraint',
    ),
  )
));

export const functionalFormsInvolvingLogs = createDoc(() => container(
  dashbox(
    doc.h2`Functional Forms Involving Logs`,
    doc.p`
      Linear regression is linear in the sense it is linear in
      parameters (unforunately I don't know what that means), and
      more simply that can be put there is a linear relation between
      the either side of the regression. This isn't to say you can't
      represent nonlinear relations.
    `,
    doc.p`
      If you have a relation between the logarithm of your
      dependent variable with independent variable or vice versa
      you can simply create a new variable for that logarithm
      and use it in computing your coefficents. Same goes for
      exponents or anything else of that nature. See the table
      below for some names of common functional forms.
    `,
    text.ul.compact.of(
      doc.li`log level is sometimes called semi-elasticity`,
      doc.li`log log is the elasticity of y with respect to x`,
      doc.li`However I am unsure if there are other names for the rest`,
    ),
  ),
  TABLES.logFunctionalForms,
));

export const assumptionsOfUnbiasednessInOLS = createDoc(() => dashbox(
  container(
    doc.h2`Assumptions of Unbiasednesss of OLS`,
    doc.p`
      These are also called the ${doc.b`Gauss Markov Assumptions`}
      they can vary between multiple linear regression. But in order
      for the model to be unbiased these assumptions must be true.
      You can also think of these as a check list to evaluate the
      biasness of a model.
    `,
  ),
  twoColumns(
    container(
      doc.h4`SLR 1 - Linear in Parameters`,
      doc.p`
        Within population model the dependent variable y
        is related to the independent variable in a linear
        fashion.
      `,
    ),
    container(
      doc.h4`SLR 2 - Random Sampling`,
      doc.p`We have a random sample of the population.`,
    ),
  ),
  twoColumns(
    container(
      doc.h4`SLR 3 - Sample Variation`,
      doc.p`Within the sample, there is more than one value for a variable`,
      doc.figure(mathml.slrAssumptions.slr3, 'SLR 3'),
    ),
    container(
      doc.h4`SLR 4 - Zero Conditional Mean`,
      doc.p`The error has an expected value of zero for any given explanatory variable`,
      doc.figure(mathml.slrAssumptions.slr4, 'SLR 4'),
    ),
  ),
  container(
    doc.h4`SLR 5 - Homoskedasticity`,
    doc.p`
      The error u has the same variance given any value of
      the explanatory variable. Meaning at any point along
      the line of best fit variance is identical for the
      error rate.
    `,
    doc.figure(mathml.slrAssumptions.slr5, 'SLR 5'),
  ),
  container(
    doc.h3`Theorum for unbiassness of the OLS`,
    todo({}, 'READ page 43'),
  ),
  container(
    doc.h3`Analysis of Simple Linear Regression`,
    todo({}, 'Explain how this is useful and important'),
    twoColumns(
      doc.figure(mathml.simpleLinearRegressionAnalysis.varB1EstA, 'Constraint A, Sampling variance of estimators'),
      doc.figure(mathml.simpleLinearRegressionAnalysis.varB1EstB, 'Constraint B, Sampling variance of estimators'),
    ),
    doc.figure(
      mathml.simpleLinearRegressionAnalysis.varB1EstC,
      'Given the above assumptions this should hold',
    ),
  ),
));

export const summary = createDoc(() => dashbox(
  doc.h2`Conclusion`,
  doc.p`main points are...`,
  note(
    'Population regression',
    'Sample regression',
    'Estimation',
    'Fitted values',
    'Regression risduals',
    'Understand why estimated parameteres are (ex-ante) random variables',
  ),
));
